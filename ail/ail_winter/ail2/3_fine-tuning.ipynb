{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune pre-trained networks\n",
    "We will fine-tune a pre-trained network (CaffeNet) on the Caltech-101 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Caltech-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "\n",
    "# fix the seed to have same training and testing samples\n",
    "random.seed(20) \n",
    "\n",
    "# 30 training samples per class and at most 20 testing samples per class\n",
    "training_sample=30\n",
    "\n",
    "dataset_dir = '{}/Caltech101/101_ObjectCategories'.format(os.getcwd())\n",
    "labels = os.listdir(dataset_dir)\n",
    "train_x, train_y, test_x, test_y = [], [], [], []\n",
    "\n",
    "total = 0\n",
    "for c, category in enumerate(labels):\n",
    "    files = os.listdir('{}/{}'.format(dataset_dir, category))\n",
    "    total += len(files)\n",
    "    random.shuffle(files)\n",
    "    for img in files[:training_sample]:\n",
    "        train_x.append('{}/{}/{}'.format(dataset_dir, category, img))\n",
    "    train_y += [c for _ in range(len(files[:training_sample]))]\n",
    "    for img in files[training_sample:training_sample + 20]: \n",
    "        test_x.append('{}/{}/{}'.format(dataset_dir, category, img))\n",
    "    test_y += [c for _ in range(len(files[training_sample: training_sample + 20]))]\n",
    "\n",
    "print('Total images: {}'.format(total))\n",
    "print('Train images: {}: {}'.format(len(train_x), len(train_y)))\n",
    "print('Validation images: {}: {}'.format(len(test_x), len(test_y)))\n",
    "# random.shuffle(train_list) # Be sure to shuffle training images (otherwise fine-tuning will fail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare preprocessing and data-augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, chainer\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from scipy.misc import imresize\n",
    "\n",
    "\n",
    "# aspect ratio is kept after resizing\n",
    "def resize_image(img, minimum_length=256):\n",
    "        y, x = img.shape[:2]\n",
    "        # keep aspect ratio\n",
    "        if y <= x:\n",
    "            scale = float(minimum_length) / y\n",
    "            sizes = (minimum_length, int(scale * x))\n",
    "        else:\n",
    "            scale = float(minimum_length) / x\n",
    "            sizes = (int(scale * y), minimum_length)\n",
    "        # If grey picture\n",
    "        if img.ndim == 2:\n",
    "            img = np.tile(img[:, :, np.newaxis], (1, 1, 3))\n",
    "        return imresize(img, sizes, interp='bilinear', mode='RGB')\n",
    "\n",
    "    \n",
    "def crop_center(img, sizes=(224, 224)):\n",
    "        y, x, channel = img.shape\n",
    "        center_y, center_x = int(y / 2), int(x / 2)\n",
    "        frame_y, frame_x = sizes\n",
    "        up, down = -int((frame_y + 1) / 2), int(frame_y / 2)\n",
    "        left, right = -int((frame_x + 1) / 2), int(frame_x / 2)\n",
    "        start_h, end_h = max(center_y + up, 0), min(center_y + down, y)\n",
    "        start_w, end_w = max(center_x + left, 0), min(center_x + right, x)\n",
    "        return img[start_h:end_h, start_w:end_w]\n",
    "\n",
    "\n",
    "def crop_randomly(img, sizes=(224, 224)):\n",
    "        y, x, channel = img.shape\n",
    "        length_y, length_x = sizes\n",
    "        # pick random number\n",
    "        keypoint_y = np.random.randint(1, y - length_y + 1)\n",
    "        keypoint_x = np.random.randint(1, x - length_x + 1)\n",
    "        start_y = keypoint_y\n",
    "        end_y = keypoint_y + length_y\n",
    "        start_x = keypoint_x\n",
    "        end_x = keypoint_x + length_x\n",
    "        return img[start_y: end_y, start_x: end_x]\n",
    "\n",
    "\n",
    "class PreprocessedDataset(chainer.dataset.DatasetMixin):\n",
    "\n",
    "    def __init__(self, x, y, crop_size=(224, 224), resize=(256, 512), horizontal_flip=True, test=False, gpu=-1):\n",
    "        self.x, self.y = x, y\n",
    "        self.crop_size = crop_size\n",
    "        self.resize = resize\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.test = test\n",
    "        self.gpu = gpu\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        # Load image\n",
    "        img = io.imread(self.x[i])\n",
    "        # Resize image in the range\n",
    "        img = resize_image(img, minimum_length=int(np.random.randint(*self.resize)))\n",
    "        if self.test:\n",
    "            # Crop center for test\n",
    "            img = crop_center(img, sizes=self.crop_size)\n",
    "        else:\n",
    "            # Crop randomly\n",
    "            img = crop_randomly(img, sizes=self.crop_size)\n",
    "            if self.horizontal_flip and np.random.rand() >= 0.5:\n",
    "                # Horizontal filp with 0.5\n",
    "                img = cv2.flip(img, 1)\n",
    "        # To BGR\n",
    "        img = img[:, :, ::-1]\n",
    "        # Subtract mean\n",
    "        img = np.array(img, dtype=np.float32) -np.array([103.063,  115.903,  123.152], dtype=np.float32)\n",
    "        # (channel , height, width)\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        # Label\n",
    "        t = np.array(self.y[i], dtype=np.int32)\n",
    "        return img, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import initializers\n",
    "import chainer.links as L\n",
    "\n",
    "\n",
    "class FineTuneResNet(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, path, layer):\n",
    "        super(FineTuneResNet, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.resnet = chainer.links.model.vision.resnet.ResNetLayers(path,  layer)\n",
    "            self.linear = L.Linear(2048, 102)\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        feature = self.resnet(x, layers=['pool5'])['pool5']\n",
    "        h = self.linear(feature)\n",
    "        loss = F.softmax_cross_entropy(h, t)\n",
    "        chainer.report({'loss': loss, 'accuracy': F.accuracy(h, t)}, self)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Start fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "train_batchsize=32\n",
    "test_batchsize=32\n",
    "\n",
    "# Define ResNet and load weights\n",
    "model = FineTuneResNet('/root/userspace/readonly/chapter1/resnet_50.caffemodel',  50)\n",
    "\n",
    "# Send to gpu\n",
    "model.to_gpu(0)\n",
    "\n",
    "# Prepare dataset\n",
    "train = PreprocessedDataset(train_x, train_y, crop_size=(224, 224), resize=(256, 257), horizontal_flip=True, test=False, gpu=0)\n",
    "val = PreprocessedDataset(test_x, test_y, crop_size=(224, 224), resize=(256, 257), horizontal_flip=False, test=True, gpu=0)\n",
    "train_iter = chainer.iterators.SerialIterator(train, train_batchsize, repeat=True, shuffle=True)\n",
    "val_iter = chainer.iterators.SerialIterator(val, test_batchsize, repeat=False, shuffle=False)\n",
    "\n",
    "# Set up an optimizer: Momentum SGD\n",
    "optimizer = chainer.optimizers.MomentumSGD(lr=0.001, momentum=0.9)\n",
    "optimizer.setup(model)\n",
    "# Weight decay\n",
    "weight_decay = chainer.optimizer.WeightDecay(5.0e-4)\n",
    "optimizer.add_hook(weight_decay)\n",
    "\n",
    "\n",
    "# Set up a trainer\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=0)\n",
    "trainer = training.Trainer(updater, (10, 'epoch'))\n",
    "val_interval = 100, 'iteration'\n",
    "log_interval = 100, 'iteration'\n",
    "trainer.extend(extensions.LogReport(trigger=log_interval))\n",
    "trainer.extend(extensions.Evaluator(val_iter, model, device=0), trigger=val_interval)\n",
    "trainer.extend(extensions.PrintReport([\n",
    "        'epoch', 'iteration', 'main/loss', 'validation/main/loss',\n",
    "        'main/accuracy', 'validation/main/accuracy',\n",
    "    ]), trigger=log_interval)\n",
    "trainer.extend(extensions.ProgressBar(update_interval=10))\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
